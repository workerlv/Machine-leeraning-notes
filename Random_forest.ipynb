{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHRMPJ6un9ZRyaS0hiyo29"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Random forest"],"metadata":{"id":"PVf34M9cLYJJ"}},{"cell_type":"markdown","source":["Random forests are a type of ensemble model, which means that they are made up of many individual models that work together to make predictions. In a random forest, these individual models are decision trees.\n","\n","A decision tree is a flowchart-like tree structure that makes a prediction based on the values of the features in an input data point. Each internal node in the tree represents a \"test\" on a feature, and each leaf node represents a class label. In a random forest, many decision trees are trained on different subsets of the training data and then the predictions of all the individual trees are combined to make the final prediction.\n","\n","There are several reasons why the random forest model is a good choice for many tasks:\n","\n","* Decision trees are simple and easy to understand, so they are easy to explain to a non-technical audience.\n","\n","* Random forests can handle high-dimensional data and are resistant to overfitting, which makes them a good choice for many real-world tasks.\n","\n","* Random forests can be used for both classification and regression tasks.\n","\n","* Random forests are fast to train and make predictions, which makes them useful for large-scale tasks.\n","\n","* Random forests are highly flexible and can be used with a wide variety of features, including continuous and categorical features."],"metadata":{"id":"5myodCXWNFvz"}},{"cell_type":"markdown","source":["## Decision tree from previous"],"metadata":{"id":"SyyQPKQsNZL8"}},{"cell_type":"code","source":["def entropy(y):\n","    hist = np.bincount(y)\n","    ps = hist / len(y)\n","    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n","\n","\n","class Node:\n","    def __init__(\n","        self, feature=None, threshold=None, left=None, right=None, *, value=None\n","    ):\n","        self.feature = feature\n","        self.threshold = threshold\n","        self.left = left\n","        self.right = right\n","        self.value = value\n","\n","    def is_leaf_node(self):\n","        return self.value is not None\n","\n","\n","class DecisionTree:\n","    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n","        self.min_samples_split = min_samples_split\n","        self.max_depth = max_depth\n","        self.n_feats = n_feats\n","        self.root = None\n","\n","    def fit(self, X, y):\n","        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n","        self.root = self._grow_tree(X, y)\n","\n","    def predict(self, X):\n","        return np.array([self._traverse_tree(x, self.root) for x in X])\n","\n","    def _grow_tree(self, X, y, depth=0):\n","        n_samples, n_features = X.shape\n","        n_labels = len(np.unique(y))\n","\n","        # stopping criteria\n","        if (\n","            depth >= self.max_depth\n","            or n_labels == 1\n","            or n_samples < self.min_samples_split\n","        ):\n","            leaf_value = self._most_common_label(y)\n","            return Node(value=leaf_value)\n","\n","        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n","\n","        # greedily select the best split according to information gain\n","        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n","\n","        # grow the children that result from the split\n","        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n","        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n","        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n","        return Node(best_feat, best_thresh, left, right)\n","\n","    def _best_criteria(self, X, y, feat_idxs):\n","        best_gain = -1\n","        split_idx, split_thresh = None, None\n","        for feat_idx in feat_idxs:\n","            X_column = X[:, feat_idx]\n","            thresholds = np.unique(X_column)\n","            for threshold in thresholds:\n","                gain = self._information_gain(y, X_column, threshold)\n","\n","                if gain > best_gain:\n","                    best_gain = gain\n","                    split_idx = feat_idx\n","                    split_thresh = threshold\n","\n","        return split_idx, split_thresh\n","\n","    def _information_gain(self, y, X_column, split_thresh):\n","        # parent loss\n","        parent_entropy = entropy(y)\n","\n","        # generate split\n","        left_idxs, right_idxs = self._split(X_column, split_thresh)\n","\n","        if len(left_idxs) == 0 or len(right_idxs) == 0:\n","            return 0\n","\n","        # compute the weighted avg. of the loss for the children\n","        n = len(y)\n","        n_l, n_r = len(left_idxs), len(right_idxs)\n","        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n","        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n","\n","        # information gain is difference in loss before vs. after split\n","        ig = parent_entropy - child_entropy\n","        return ig\n","\n","    def _split(self, X_column, split_thresh):\n","        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n","        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n","        return left_idxs, right_idxs\n","\n","    def _traverse_tree(self, x, node):\n","        if node.is_leaf_node():\n","            return node.value\n","\n","        if x[node.feature] <= node.threshold:\n","            return self._traverse_tree(x, node.left)\n","        return self._traverse_tree(x, node.right)\n","\n","    def _most_common_label(self, y):\n","        counter = Counter(y)\n","        most_common = counter.most_common(1)[0][0]\n","        return most_common"],"metadata":{"id":"XrmLBfoQNYZJ","executionInfo":{"status":"ok","timestamp":1672775887303,"user_tz":-120,"elapsed":231,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Random forest"],"metadata":{"id":"fBy_DTCDNoiv"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"wlyvWAzULPmQ","executionInfo":{"status":"ok","timestamp":1672775928579,"user_tz":-120,"elapsed":255,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"outputs":[],"source":["from collections import Counter\n","\n","import numpy as np\n","\n","\n","def bootstrap_sample(X, y):\n","    n_samples = X.shape[0]\n","    idxs = np.random.choice(n_samples, n_samples, replace=True)\n","    return X[idxs], y[idxs]\n","\n","\n","def most_common_label(y):\n","    counter = Counter(y)\n","    most_common = counter.most_common(1)[0][0]\n","    return most_common\n","\n","\n","class RandomForest:\n","    def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_feats=None):\n","        self.n_trees = n_trees\n","        self.min_samples_split = min_samples_split\n","        self.max_depth = max_depth\n","        self.n_feats = n_feats\n","        self.trees = []\n","\n","    def fit(self, X, y):\n","        self.trees = []\n","        for _ in range(self.n_trees):\n","            tree = DecisionTree(\n","                min_samples_split=self.min_samples_split,\n","                max_depth=self.max_depth,\n","                n_feats=self.n_feats,\n","            )\n","            X_samp, y_samp = bootstrap_sample(X, y)\n","            tree.fit(X_samp, y_samp)\n","            self.trees.append(tree)\n","\n","    def predict(self, X):\n","        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n","        tree_preds = np.swapaxes(tree_preds, 0, 1)\n","        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n","        return np.array(y_pred)"]},{"cell_type":"code","source":["# Testing\n","if __name__ == \"__main__\":\n","    # Imports\n","    from sklearn import datasets\n","    from sklearn.model_selection import train_test_split\n","\n","    def accuracy(y_true, y_pred):\n","        accuracy = np.sum(y_true == y_pred) / len(y_true)\n","        return accuracy\n","\n","    data = datasets.load_breast_cancer()\n","    X = data.data\n","    y = data.target\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2, random_state=1234\n","    )\n","\n","    clf = RandomForest(n_trees=3, max_depth=10)\n","\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","    acc = accuracy(y_test, y_pred)\n","\n","    print(\"Accuracy:\", acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hKbXsKNWNxyJ","executionInfo":{"status":"ok","timestamp":1672776439651,"user_tz":-120,"elapsed":6540,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}},"outputId":"17c306db-d347-40f6-9a01-c93499e41145"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9210526315789473\n"]}]},{"cell_type":"markdown","source":["## sklearn"],"metadata":{"id":"pwFHO2T-N5om"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_classification\n","\n","clf_ = RandomForestClassifier(n_estimators=3, max_depth=10, random_state=0)\n","clf_.fit(X_train, y_train)\n","\n","y_pred_ = clf_.predict(X_test)\n","\n","acc_ = accuracy(y_test, y_pred_)\n","print(\"Accuracy:\", acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Ta9THlfN7in","executionInfo":{"status":"ok","timestamp":1672776439652,"user_tz":-120,"elapsed":13,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}},"outputId":"0ddf6385-4259-4637-e973-51618202ac8e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9210526315789473\n"]}]}]}