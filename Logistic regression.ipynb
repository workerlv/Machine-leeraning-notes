{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOerdNFUsGc6WP6o78Dc14J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Logistic regression"],"metadata":{"id":"ychvWtWSuZUM"}},{"cell_type":"markdown","source":["## numpy"],"metadata":{"id":"TjBFSgZDugHH"}},{"cell_type":"markdown","source":["With logistic regression we get probability from 0 to 1\n","* Approxiamtion: \n","$$ f(w, b) = wx + b $$\n","$$ \\hat{y} = h_{\\Theta}(x) = \\frac{1}{1+e^{-wx+b}} $$\n","* Sigmoid function:\n","$$ s(x) = \\frac{1}{1+e^{-x}} $$\n","\n","* Cost function (cross entropy):\n","$$ J(w, b) = J(Θ) = \\frac{1}{N}\\sum_{i=1}^{n}[y^ilog(h_{Θ}(x^{i}))+(1-y^{i})log(1-h_{Θ}(x^{i}))] $$\n","\n","* Update rules:\n","$$ w=w-\\alpha*dw $$\n","$$ b=b-\\alpha*db $$\n","where:\n","$$ \\alpha - learning\\; rate $$\n","\n","* Derivative:\n","$$ J'(Θ) = \\begin{bmatrix} \\frac{dJ}{dw}\\\\ \\frac{dJ}{db}\\end{bmatrix} = [....] =  \\begin{bmatrix} \\frac{1}{N}\\sum 2x_{i}(\\hat{y} - y_{i}) \\\\ \\frac{1}{N} \\sum 2(\\hat{y} - y_{i}) \\end{bmatrix} $$ \n"],"metadata":{"id":"PzXwh_CfupEv"}},{"cell_type":"markdown","source":["Logistic regression is a machine learning algorithm that is used for classification problems, where the goal is to predict a categorical target variable. It is called \"logistic\" because it uses a logistic function to make predictions.\n","\n","The logistic function is a sigmoid function that maps the input to a value between 0 and 1, which can be interpreted as a probability. For example, if the logistic function outputs a value of 0.7 for a given input, that input can be classified as belonging to the positive class with a probability of 0.7.\n","\n","To perform logistic regression, you need a dataset with a set of input features and a binary target variable (i.e., a variable that has only two possible values). You can then use a variety of techniques, such as gradient descent or Newton's method, to find the optimal coefficients for the input features. Once you have found the optimal coefficients, you can use the logistic regression model to make predictions on new data by plugging in the input features and using the learned coefficients to predict the probability of the target variable.\n","\n","Logistic regression is a simple and widely used machine learning algorithm that is well-suited for binary classification problems. It is a good choice for many applications, including predicting whether an email is spam, whether a customer will churn, or whether a patient has a certain disease."],"metadata":{"id":"_3pVxlFBW_rJ"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"aImvmn21uFEV","executionInfo":{"status":"ok","timestamp":1671486567985,"user_tz":-120,"elapsed":1216,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets"]},{"cell_type":"markdown","source":["## very detailed"],"metadata":{"id":"mdCtgfde0Xlk"}},{"cell_type":"code","source":["def sigmoid_func(x):\n","    return 1 / (1 + np.exp(-x))"],"metadata":{"id":"8IEsNboP3Pwo","executionInfo":{"status":"ok","timestamp":1671486568405,"user_tz":-120,"elapsed":424,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Some sample data from sklearn\n","bc = datasets.load_breast_cancer()\n","X, y = bc.data, bc.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"],"metadata":{"id":"MpkuAl9449UQ","executionInfo":{"status":"ok","timestamp":1671486568406,"user_tz":-120,"elapsed":17,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(X_train.shape)\n","print(y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipAvN_I05s67","executionInfo":{"status":"ok","timestamp":1671486568406,"user_tz":-120,"elapsed":16,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}},"outputId":"e0845d65-122c-486f-f3d6-f57d0bf56caa"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["(455, 30)\n","(455,)\n"]}]},{"cell_type":"code","source":["# init parameters\n","learning_rate = 0.0001\n","n_iters = 1000\n","n_samples, n_features = X.shape\n","\n","weights = np.zeros(n_features)\n","bias = 0"],"metadata":{"id":"8IfliwDX5UYm","executionInfo":{"status":"ok","timestamp":1671486568406,"user_tz":-120,"elapsed":5,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# gradient descent\n","for _ in range(n_iters):\n","    # approximate y with linear combination of weights and x, plus bias\n","    linear_model = np.dot(X_train, weights) + bias\n","    \n","    # apply sigmoid function\n","    y_predicted = sigmoid_func(linear_model)\n","\n","    # compute gradients\n","    dw = (1 / n_samples) * np.dot(X_train.T, (y_predicted - y_train))\n","    db = (1 / n_samples) * np.sum(y_predicted - y_train)\n","    \n","    # update parameters\n","    weights -= learning_rate * dw\n","    bias -= learning_rate * db"],"metadata":{"id":"_Ov-f2pH9Ny3","executionInfo":{"status":"ok","timestamp":1671486568407,"user_tz":-120,"elapsed":6,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# predict\n","linear_model = np.dot(X_test, weights) + bias\n","y_predicted = sigmoid_func(linear_model)\n","y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n","predictions = np.array(y_predicted_cls)"],"metadata":{"id":"lJz-YxUv9twK","executionInfo":{"status":"ok","timestamp":1671486568407,"user_tz":-120,"elapsed":6,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# accuracy\n","accuracy = np.sum(y_test == predictions) / len(y_test)\n","\n","print(\"LR classification accuracy:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55dQ0M6Y1qKY","executionInfo":{"status":"ok","timestamp":1671486568407,"user_tz":-120,"elapsed":5,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}},"outputId":"d76d4d45-7dcc-49e6-efeb-c3fb7cbf289c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["LR classification accuracy: 0.9035087719298246\n"]}]},{"cell_type":"markdown","source":["## clean version"],"metadata":{"id":"OknTaOB90PbN"}},{"cell_type":"code","source":["class LogisticRegression:\n","    def __init__(self, learning_rate=0.001, n_iters=1000):\n","        self.lr = learning_rate\n","        self.n_iters = n_iters\n","        self.weights = None\n","        self.bias = None\n","\n","    def fit(self, X, y):\n","        n_samples, n_features = X.shape\n","\n","        # init parameters\n","        self.weights = np.zeros(n_features)\n","        self.bias = 0\n","\n","        # gradient descent\n","        for _ in range(self.n_iters):\n","            # approximate y with linear combination of weights and x, plus bias\n","            linear_model = np.dot(X, self.weights) + self.bias\n","            # apply sigmoid function\n","            y_predicted = self._sigmoid(linear_model)\n","\n","            # compute gradients\n","            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n","            db = (1 / n_samples) * np.sum(y_predicted - y)\n","            # update parameters\n","            self.weights -= self.lr * dw\n","            self.bias -= self.lr * db\n","\n","    def predict(self, X):\n","        linear_model = np.dot(X, self.weights) + self.bias\n","        y_predicted = self._sigmoid(linear_model)\n","        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n","        return np.array(y_predicted_cls)\n","\n","    def _sigmoid(self, x):\n","        return 1 / (1 + np.exp(-x))\n"],"metadata":{"id":"akBru9bP0SFx","executionInfo":{"status":"ok","timestamp":1671486568407,"user_tz":-120,"elapsed":5,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Testing\n","if __name__ == \"__main__\":\n","    # Imports\n","    from sklearn.model_selection import train_test_split\n","    from sklearn import datasets\n","\n","    def accuracy(y_true, y_pred):\n","        accuracy = np.sum(y_true == y_pred) / len(y_true)\n","        return accuracy\n","\n","    bc = datasets.load_breast_cancer()\n","    X, y = bc.data, bc.target\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2, random_state=1234\n","    )\n","\n","    regressor = LogisticRegression(learning_rate=0.0001, n_iters=1000)\n","    regressor.fit(X_train, y_train)\n","    predictions = regressor.predict(X_test)\n","\n","    print(\"LR classification accuracy:\", accuracy(y_test, predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DWRbSjH0VE8","executionInfo":{"status":"ok","timestamp":1671486568701,"user_tz":-120,"elapsed":298,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}},"outputId":"f5f6494d-275a-4346-d97b-8e257899e760"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["LR classification accuracy: 0.9298245614035088\n"]}]},{"cell_type":"markdown","source":["## sklearn"],"metadata":{"id":"P3Ci3Jtfc2mN"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets"],"metadata":{"id":"dR1fGk9Xc40W","executionInfo":{"status":"ok","timestamp":1671486568702,"user_tz":-120,"elapsed":6,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["bc_ = datasets.load_breast_cancer()\n","X_, y_ = bc_.data, bc_.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=1234)"],"metadata":{"id":"_RhbmjP5dDTJ","executionInfo":{"status":"ok","timestamp":1671486568702,"user_tz":-120,"elapsed":5,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["clf = LogisticRegression(random_state=0)\n","clf.fit(X_train, y_train)\n","y_hat = clf.predict(X_test)\n","\n","clf.predict_proba(X_test)\n","clf.score(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z82ygZ8DdQ5_","executionInfo":{"status":"ok","timestamp":1671486568988,"user_tz":-120,"elapsed":290,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}},"outputId":"28fb86dc-d7ec-4168-d9d0-07fa802e8926"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"execute_result","data":{"text/plain":["0.9385964912280702"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["accuracy_ = np.sum(y_test == y_hat) / len(y_test)\n","\n","print(\"LR classification accuracy:\", accuracy_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xnlkfL_Gdirt","executionInfo":{"status":"ok","timestamp":1671486568988,"user_tz":-120,"elapsed":6,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}},"outputId":"233f2859-a6bc-4077-94ed-706fee010a93"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["LR classification accuracy: 0.9385964912280702\n"]}]}]}