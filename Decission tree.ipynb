{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOrxgBM/C4z2SOrXGWSNhhD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Decission tree"],"metadata":{"id":"V3aIjYGNNHoX"}},{"cell_type":"markdown","source":["Entropy:\n","$$ E = -∑ p(X) * log_{2} (p(X)) $$\n","$$ p(X) = \\frac{\\#x}{n} $$\n","\n","Example:\n","$$ S = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] $$\n","$$ E = - \\frac{5}{10} * log_{2}(\\frac{5}{10}) - \\frac{5}{10} * log_{2}(\\frac{5}{10}) = -0.5log_{2}(-0.5)-0.5log_{2}(-0.5) = 1 $$\n","$$ E = -0.5 * (-1) - 0.5 * (-1) = 1 $$\n","\n","Information gain:\n","$$ IG = E(parent) - [weighted \\ average] * E(children) $$\n","\n","Example:\n","$$ S = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] $$\n","$$ S1 = [0, 0, 1, 1, 1, 1, 1] $$\n","$$ S2 = [0, 0, 0] $$\n","$$ IG = E(S) - [\\frac{7}{10} * E(S1) + \\frac{3}{10} * E(S2)] $$\n","$$ IG = 1 - [\\frac{7}{10} * 0.863 + \\frac{3}{10} * 0] = 0.395 $$\n","\n","Approach:\n","Train algorithm := Build the tree\n","• Start at the top node and at each node select the best split based on the best information gain.\n","• Greedy search: Loop over all features and over all thresholds (all possible feature values).\n","Save the best split feature and split threshold at each node.\n","• Build the tree recursively.\n","• Apply some stopping criteria to stop growing\n","e.g. here: maximum depth, minimum samples at node, no more class distribution in node.\n","• When we have a leaf node, store the most common class label of this node\n","\n","\n","Predict := Traverse tree\n","• Traverse the treelrecursively.\n","• At each node look at the best split feature of the test feature vector x and go left or right\n","depending on [feature_idx] <= threshold\n","• When we reach the leaf node we return the stored most common class label"],"metadata":{"id":"uG2tWb2hNRV7"}},{"cell_type":"markdown","source":["In machine learning, entropy is often used as a measure of the uncertainty or randomness of a system or dataset. In this context, entropy is often used in the context of decision trees, where it is used to measure the impurity of a set of data.\n","\n","In a decision tree, each internal node represents a test on an attribute, and each leaf node represents a class label. The entropy of a node is a measure of the impurity of the data at that node, with a low entropy indicating that the data is pure (i.e., all of the instances at that node belong to the same class) and a high entropy indicating that the data is impure (i.e., the instances at that node belong to multiple classes).\n","\n","Entropy is used in decision tree learning to determine the best attribute to split on at each node. The attribute with the highest information gain (i.e., the attribute that reduces entropy the most) is chosen as the splitting attribute.\n","\n","Entropy can also be used in other machine learning algorithms, such as clustering and feature selection, to measure the randomness or disorder of a dataset. In these contexts, entropy is used to measure the quality of a clustering or the importance of a feature in predicting the target variable."],"metadata":{"id":"GgW4HgrEOmme"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"4sp0-wf7Mk_A","executionInfo":{"status":"ok","timestamp":1671969451102,"user_tz":-120,"elapsed":971,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"outputs":[],"source":["from collections import Counter\n","import numpy as np"]},{"cell_type":"markdown","source":["## clean version"],"metadata":{"id":"gnfCRHmEEIdn"}},{"cell_type":"code","source":["def entropy(y):\n","    hist = np.bincount(y)\n","    ps = hist / len(y)\n","    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n","\n","\n","class Node:\n","    def __init__(\n","        self, feature=None, threshold=None, left=None, right=None, *, value=None\n","    ):\n","        self.feature = feature\n","        self.threshold = threshold\n","        self.left = left\n","        self.right = right\n","        self.value = value\n","\n","    def is_leaf_node(self):\n","        return self.value is not None\n","\n","\n","class DecisionTree:\n","    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n","        self.min_samples_split = min_samples_split\n","        self.max_depth = max_depth\n","        self.n_feats = n_feats\n","        self.root = None\n","\n","    def fit(self, X, y):\n","        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n","        self.root = self._grow_tree(X, y)\n","\n","    def predict(self, X):\n","        return np.array([self._traverse_tree(x, self.root) for x in X])\n","\n","    def _grow_tree(self, X, y, depth=0):\n","        n_samples, n_features = X.shape\n","        n_labels = len(np.unique(y))\n","\n","        # stopping criteria\n","        if (\n","            depth >= self.max_depth\n","            or n_labels == 1\n","            or n_samples < self.min_samples_split\n","        ):\n","            leaf_value = self._most_common_label(y)\n","            return Node(value=leaf_value)\n","\n","        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n","\n","        # greedily select the best split according to information gain\n","        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n","\n","        # grow the children that result from the split\n","        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n","        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n","        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n","        return Node(best_feat, best_thresh, left, right)\n","\n","    def _best_criteria(self, X, y, feat_idxs):\n","        best_gain = -1\n","        split_idx, split_thresh = None, None\n","        for feat_idx in feat_idxs:\n","            X_column = X[:, feat_idx]\n","            thresholds = np.unique(X_column)\n","            for threshold in thresholds:\n","                gain = self._information_gain(y, X_column, threshold)\n","\n","                if gain > best_gain:\n","                    best_gain = gain\n","                    split_idx = feat_idx\n","                    split_thresh = threshold\n","\n","        return split_idx, split_thresh\n","\n","    def _information_gain(self, y, X_column, split_thresh):\n","        # parent loss\n","        parent_entropy = entropy(y)\n","\n","        # generate split\n","        left_idxs, right_idxs = self._split(X_column, split_thresh)\n","\n","        if len(left_idxs) == 0 or len(right_idxs) == 0:\n","            return 0\n","\n","        # compute the weighted avg. of the loss for the children\n","        n = len(y)\n","        n_l, n_r = len(left_idxs), len(right_idxs)\n","        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n","        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n","\n","        # information gain is difference in loss before vs. after split\n","        ig = parent_entropy - child_entropy\n","        return ig\n","\n","    def _split(self, X_column, split_thresh):\n","        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n","        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n","        return left_idxs, right_idxs\n","\n","    def _traverse_tree(self, x, node):\n","        if node.is_leaf_node():\n","            return node.value\n","\n","        if x[node.feature] <= node.threshold:\n","            return self._traverse_tree(x, node.left)\n","        return self._traverse_tree(x, node.right)\n","\n","    def _most_common_label(self, y):\n","        counter = Counter(y)\n","        most_common = counter.most_common(1)[0][0]\n","        return most_common"],"metadata":{"id":"0PbZdiSJEKH1","executionInfo":{"status":"ok","timestamp":1671969824918,"user_tz":-120,"elapsed":2,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Imports\n","    from sklearn import datasets\n","    from sklearn.model_selection import train_test_split\n","\n","    def accuracy(y_true, y_pred):\n","        accuracy = np.sum(y_true == y_pred) / len(y_true)\n","        return accuracy\n","\n","    data = datasets.load_breast_cancer()\n","    X, y = data.data, data.target\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2, random_state=1234\n","    )\n","\n","    clf = DecisionTree(max_depth=10)\n","    clf.fit(X_train, y_train)\n","\n","    y_pred = clf.predict(X_test)\n","    acc = accuracy(y_test, y_pred)\n","\n","    print(\"Accuracy:\", acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5XB3eUUdETMl","executionInfo":{"status":"ok","timestamp":1671969833068,"user_tz":-120,"elapsed":3801,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}},"outputId":"b23c0ff4-bf0a-4d70-ecf6-b22633cb7ad8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9385964912280702\n"]}]},{"cell_type":"markdown","source":["## sklearn"],"metadata":{"id":"WvgL5QIIK5DC"}},{"cell_type":"code","source":["from sklearn import tree\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","\n","data = datasets.load_breast_cancer()\n","X, y = data.data, data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=1234\n",")\n","\n","\n","# iris = load_iris()\n","# X, y = iris.data, iris.target\n","clf = tree.DecisionTreeClassifier(max_depth=10, min_samples_split=2)\n","clf = clf.fit(X_train, y_train)\n","\n","y_pred = clf.predict(X_test)\n","acc = accuracy(y_test, y_pred)\n","\n","print(\"Accuracy:\", np.sum(y_test == y_pred) / len(y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7cIl4R8K7J0","executionInfo":{"status":"ok","timestamp":1671970533296,"user_tz":-120,"elapsed":243,"user":{"displayName":"Arturs Vitins","userId":"16068103353402116777"}},"outputId":"fbb022c9-2164-408b-e01f-571987d5d642"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9122807017543859\n"]}]}]}